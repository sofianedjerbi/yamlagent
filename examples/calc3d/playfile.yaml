version: 1

# Import shared configuration from .play/ directory
imports:
  - .play/tools.yaml
  - .play/agents.yaml

# Core TDD workflows - test, code, review cycle built-in
#
# Validation commands are enabled with "uv run pytest" for this Python project.
#
tasks:
  # Feature Workflow: Architect -> TDD with validation
  - id: feature
    description: "Full feature: Technical spec -> Tests -> Implement -> Refactor -> Review"
    working_dir: "."
    files:
      read:
        - "**/*"
    steps:
      # Step 1: Create technical specification
      - id: spec
        name: "Create technical specification"
        agent:
          use: architect
          with:
            prompt: "Create technical specification for: {{ inputs.prompt }}"

      # Step 2: Write tests first based on spec (RED phase)
      - id: tests
        name: "Write tests (RED phase)"
        agent:
          use: tester
          with:
            prompt: "Create tests for the specified feature. Cover API contracts, data models, and core behavior."
          context_from:
            - spec  # Get architect's technical specification

      # Step 3: Implement to make tests pass (GREEN phase) with validation
      - id: implementation
        name: "Implement feature (GREEN phase)"
        agent:
          use: coder
          with:
            prompt: |
              ORIGINAL REQUEST: "{{ inputs.prompt }}"

              Implement the COMPLETE feature from the technical specification provided above.
              Every component, file, and interface mentioned in the spec must be fully implemented.

              The technical specification is your source of truth - implement everything it describes.
              Pay attention to the original user's intent and any priorities they emphasized.

              Tests provide validation checkpoints to ensure correctness, but your goal is
              to deliver the complete feature as specified, not just pass the tests.
          context_from:
            - spec   # Get technical specification
            - tests  # Get test requirements
        validate:
          command: "uv run pytest"
          max_retries: 3
          continue_on_failure: false

      # Step 4: Refactor with best practices (REFACTOR phase)
      - id: refactored
        name: "Refactor (REFACTOR phase)"
        agent:
          use: coder
          with:
            prompt: |
              Refactor the implementation for "{{ inputs.prompt }}"

              Rules:
              1. Modify existing files in-place, don't create parallel utilities
              2. Only extract utilities if code is duplicated 3+ times
              3. If you extract a utility, immediately update all call sites and delete old code
              4. Keep it simple - inline duplication is fine for 2 occurrences
              5. Don't break tests

              Focus on: breaking down long functions, improving names, removing dead code.
              Avoid: over-abstraction, unnecessary layers, premature extraction.
          context_from:
            - spec            # Original spec for reference
            - implementation  # Current implementation to refactor
        validate:
          command: "uv run pytest"
          max_retries: 2
          continue_on_failure: false

      # Step 5: Review the complete implementation
      - name: "Review implementation"
        agent:
          use: reviewer
          with:
            prompt: "Review the feature implementation. Check architecture, code quality, and whether tests cover key scenarios."
          context_from:
            - spec        # Original requirements
            - refactored  # Final implementation

  # Quick: Implement -> Test -> Review with validation
  - id: code
    description: "Implement feature -> Create tests -> Review (with automatic validation)"
    working_dir: "."
    files:
      read:
        - "**/*"
    steps:
      # Step 1: Implement the feature with best practices
      - name: "Implement feature"
        agent:
          use: coder
          with:
            prompt: |
              REQUEST: "{{ inputs.prompt }}"

              Implement the complete feature following best practices (SOLID, DRY, clean code).
              Make sure the implementation is thorough and addresses all aspects of the request.

              Consider edge cases, error handling, and maintainability.

      # Step 2: Create tests with validation
      - name: "Create tests"
        agent:
          use: tester
          with:
            prompt: "Create simple, efficient tests covering the implementation. Test happy path, edge cases, and errors."
        validate:
          command: "uv run pytest"
          max_retries: 2
          continue_on_failure: false

      # Step 3: Review implementation and tests
      - name: "Review code"
        agent:
          use: reviewer
          with:
            prompt: "Review the implementation and tests for quality and best practices."

  # Bug Fix Workflow: Root cause -> Fix -> Validate -> Review
  - id: bugfix
    description: "Find root cause -> Fix with best practices -> Validate -> Review"
    working_dir: "."
    files:
      read:
        - "**/*"
    steps:
      # Step 1: Find root cause (100% sure before fixing) - dedicated debugger
      - name: "Find root cause"
        agent:
          use: debugger
          with:
            prompt: "Investigate and find the ROOT CAUSE of: {{ inputs.prompt }}"

      # Step 2: Fix the root cause with best practices and validation
      - name: "Fix bug"
        agent:
          use: coder
          with:
            prompt: |
              BUG REPORT: "{{ inputs.prompt }}"

              Fix the root cause following best practices.
              Ensure the fix is minimal, targeted, and doesn't introduce new issues.

              Based on the debugger's root cause analysis, implement a complete fix that addresses
              the underlying issue, not just the symptoms.
        validate:
          command: "uv run pytest"
          max_retries: 2
          continue_on_failure: false

      # Step 3: Review the bugfix
      - name: "Review bugfix"
        agent:
          use: reviewer
          with:
            prompt: "Review the bugfix: Does it address the root cause? Does it follow best practices? Any potential side effects?"